/***********************************************************************
 *                                                                     *
 *                  A U T O M L S E L E C T   P A C K A G E           *
 *                                                                     *
 *                     E V A L U A T E _ C L A S S I F I C A T I O N *
 *                                                                     *
 ***********************************************************************/

/*! version 1.0
    -------------------------------------------------------------------------
    NAME
        evaluate_classification - Evaluate Classification Model Performance

    SYNOPSIS
        evaluate_classification, ///
            target(varname) ///
            prediction(varname) ///
            [ probability(varname) ] ///
            [ save_metrics(string) ]

    DESCRIPTION
        The `evaluate_classification` command assesses the performance of a
        classification model by calculating key metrics such as Accuracy,
        Precision, Recall, F1-Score, and Area Under the ROC Curve (AUC).
        This evaluation helps in understanding the effectiveness of the trained
        model in predicting binary outcomes.

        **Key Functionalities:**
        - **Confusion Matrix Calculation:** Computes True Positives (TP),
          False Positives (FP), True Negatives (TN), and False Negatives (FN).
        - **Performance Metrics:** Calculates Accuracy, Precision, Recall, F1-Score,
          and optionally AUC if probability predictions are provided.
        - **Metrics Saving:** Optionally saves the calculated metrics to a CSV file
          for further analysis or reporting.

    OPTIONS
        target(varname)
            Specifies the name of the target (actual) variable in the dataset.
            This variable should contain binary categorical values representing
            the true class labels.

        prediction(varname)
            Specifies the name of the prediction variable in the dataset.
            This variable contains the predicted class labels generated by the
            classification model.

        probability(varname)
            (Optional) Specifies the name of the probability variable in the dataset.
            This variable should contain the predicted probabilities for the positive
            class. Providing this allows the calculation of the AUC metric.

        save_metrics(string)
            (Optional) Specifies the filename (with `.csv` extension) to which
            the calculated performance metrics will be saved. If provided, the metrics
            will be exported to the designated CSV file.

    EXAMPLES

        1. **Evaluating a Classification Model and Saving Metrics**

            Suppose you have a dataset `customer_data.dta` with the target variable `Churn`,
            the prediction variable `Churn_pred`, and the probability variable `Churn_prob`.
            You want to evaluate the model's performance and save the metrics to `churn_metrics.csv`.

            ```stata
            . evaluate_classification, ///
                  target(Churn) ///
                  prediction(Churn_pred) ///
                  probability(Churn_prob) ///
                  save_metrics("churn_metrics.csv")
            ```

            **Explanation:**
            - **Target Variable:** `Churn` (actual class labels).
            - **Prediction Variable:** `Churn_pred` (predicted class labels).
            - **Probability Variable:** `Churn_prob` (predicted probabilities for the positive class).
            - **Saving Metrics:** Exports Accuracy, Precision, Recall, F1-Score, and AUC to `churn_metrics.csv`.

        2. **Evaluating a Classification Model Without Saving Metrics**

            Consider a dataset `marketing_data.dta` with the target variable `Purchase` and the prediction variable `Purchase_pred`.
            You want to evaluate the model's performance without saving the metrics.

            ```stata
            . evaluate_classification, ///
                  target(Purchase) ///
                  prediction(Purchase_pred)
            ```

            **Explanation:**
            - **Target Variable:** `Purchase` (actual class labels).
            - **Prediction Variable:** `Purchase_pred` (predicted class labels).
            - **No Probability Variable:** AUC will not be calculated.
            - **No Saving Metrics:** Metrics will be displayed in the Stata Results window.

        3. **Evaluating a Classification Model with Probability Scores**

            You have a dataset `survey_data.dta` with the target variable `Responded`, the prediction variable `Responded_pred`,
            and the probability variable `Responded_prob`. You want to evaluate the model and save the metrics.

            ```stata
            . evaluate_classification, ///
                  target(Responded) ///
                  prediction(Responded_pred) ///
                  probability(Responded_prob) ///
                  save_metrics("responded_metrics.csv")
            ```

            **Explanation:**
            - **Target Variable:** `Responded` (actual class labels).
            - **Prediction Variable:** `Responded_pred` (predicted class labels).
            - **Probability Variable:** `Responded_prob` (predicted probabilities for the positive class).
            - **Saving Metrics:** Exports Accuracy, Precision, Recall, F1-Score, and AUC to `responded_metrics.csv`.

    NOTES
        - **Data Requirements:**
            - The target and prediction variables must exist in the dataset and contain binary categorical values.
            - If the `probability` option is used, ensure that the specified probability variable contains numeric values between 0 and 1.
            - The dataset should not contain missing values in the target, prediction, or probability variables. Handle missing data prior to evaluation.

        - **Metric Interpretation:**
            - **Accuracy:** Proportion of correctly classified instances.
            - **Precision:** Proportion of positive identifications that were actually correct.
            - **Recall (Sensitivity):** Proportion of actual positives that were correctly identified.
            - **F1-Score:** Harmonic mean of Precision and Recall, providing a balance between the two.
            - **AUC:** Measures the ability of the model to distinguish between classes. Requires probability scores.

        - **Handling Ties or Undefined Metrics:**
            - If Precision or Recall cannot be computed due to division by zero (e.g., no positive predictions), the corresponding metric will be set to missing (`.`).

        - **Performance Optimization:**
            - For large datasets, consider using `quietly` or `noisily` prefixes to control the verbosity of output during evaluation.

        - **Integration with AutoMLSelect Workflow:**
            - Typically, `evaluate_classification` is used as part of the broader `automlselect` command workflow, which includes data preprocessing, model training, evaluation, and selection.

    AUTHOR
        Developed by [Your Name or Organization], 2024.

    VERSION
        1.0

    SEE ALSO
        automlselect, automlselect_preprocess, automlselect_train, automlselect_evaluate, automlselect_select, evaluate_regression
